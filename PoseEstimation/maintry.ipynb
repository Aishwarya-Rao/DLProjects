{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\arao86\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\arao86\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\arao86\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\arao86\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\arao86\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the MoveNet model from TensorFlow Hub\n",
    "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of keypoints to body parts\n",
    "keypoint_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder',\n",
    "                  'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "                  'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
    "\n",
    "# Define the connections between keypoints to draw lines for visualization\n",
    "connections = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "               (5, 6), (5, 11), (6, 12), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform pose detection on a static image\n",
    "def detect_pose_static(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # Convert image to RGB (MoveNet expects RGB images)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Resize image to the expected input size of MoveNet\n",
    "    image_resized = tf.image.resize_with_pad(tf.expand_dims(image_rgb, axis=0), 192, 192) #192 for lightning\n",
    "    # Convert the resized image tensor to a NumPy array with dtype uint8\n",
    "    image_np = image_resized.numpy().astype(np.int32)\n",
    "    # Perform inference\n",
    "    outputs = movenet.signatures[\"serving_default\"](tf.constant(image_np))\n",
    "    # Extract the keypoints\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    # Return the keypoints\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize keypoints on a static image\n",
    "def visualize_pose_static(image_path, keypoints):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # Convert keypoints to numpy array\n",
    "    keypoints = np.array(keypoints)\n",
    "    #print(\"Shape of keypoints array:\", keypoints.shape)\n",
    "    # Ensure keypoints array has the expected shape\n",
    "    if keypoints.shape == (1, 1, 17, 3):\n",
    "        # Extract keypoints from the array\n",
    "        keypoints = keypoints[0, 0]\n",
    "        # Loop through each keypoint\n",
    "        for kp in keypoints:\n",
    "            # Extract x and y coordinates of the keypoint\n",
    "            x = int(kp[1] * image.shape[1])\n",
    "            y = int(kp[0] * image.shape[0])\n",
    "            # Draw a circle at the keypoint position\n",
    "            cv2.circle(image, (x, y), 12, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "        # Draw lines connecting keypoints\n",
    "        for connection in connections:\n",
    "            start_point = (int(keypoints[connection[0], 1] * image.shape[1]),\n",
    "                           int(keypoints[connection[0], 0] * image.shape[0]))\n",
    "            end_point = (int(keypoints[connection[1], 1] * image.shape[1]),\n",
    "                         int(keypoints[connection[1], 0] * image.shape[0]))\n",
    "            cv2.line(image, start_point, end_point, (0, 0, 255), 8)  # Increase thickness and change color to red\n",
    "        # Show the image with keypoints and lines using cv2_imshow\n",
    "        window_name = 'image'\n",
    "        cv2.imshow(window_name,image)\n",
    "        cv2.waitKey(0) \n",
    "        # closing all open windows \n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        print(\"Unexpected shape of keypoints array:\", keypoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_image_path = \"manstand.jpg\"\n",
    "# Perform pose detection on static image\n",
    "static_keypoints = detect_pose_static(static_image_path)\n",
    "visualize_pose_static(static_image_path, static_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform pose detection on an image sequence or GIF\n",
    "def detect_pose_sequence(gif_path):\n",
    "    # Load the GIF\n",
    "    gif = cv2.VideoCapture(gif_path)\n",
    "    frames = []\n",
    "    # Read frames from the GIF\n",
    "    while True:\n",
    "        ret, frame = gif.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    # Initialize an empty list to store keypoints for each frame\n",
    "    all_keypoints = []\n",
    "    # Iterate through each frame\n",
    "    for frame in frames:\n",
    "        # Convert frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Resize frame to the expected input size of MoveNet\n",
    "        frame_resized = tf.image.resize_with_pad(tf.expand_dims(frame_rgb, axis=0), 192, 192) # 192 for lightning\n",
    "        # Convert the resized frame tensor to a NumPy array with dtype uint8\n",
    "        frame_np = frame_resized.numpy().astype(np.int32)\n",
    "        # Perform inference\n",
    "        outputs = movenet.signatures[\"serving_default\"](tf.constant(frame_np))\n",
    "        # Extract the keypoints\n",
    "        keypoints = outputs['output_0'].numpy()\n",
    "        # Append keypoints to the list\n",
    "        all_keypoints.append(keypoints)\n",
    "    # Return keypoints for all frames\n",
    "    return all_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to visualize keypoints on an image sequence or GIF and create a new GIF\n",
    "def visualize_and_create_pose_sequence(gif_path, keypoints_list, output_gif_path, default_fps=10):\n",
    "    # Load the GIF\n",
    "    gif = imageio.get_reader(gif_path)\n",
    "    # Initialize list to store frames with keypoints overlay\n",
    "    frames_with_keypoints = []\n",
    "    # Loop through each frame and its corresponding keypoints\n",
    "    for frame_index, (frame, keypoints) in enumerate(zip(gif, keypoints_list)):\n",
    "        # print(frame.shape,keypoints.shape)\n",
    "        # Convert keypoints to numpy array\n",
    "        print(frame)\n",
    "        keypoints = np.array(keypoints)\n",
    "        # Ensure keypoints array has the expected shape\n",
    "        if keypoints.shape == (1, 1, 17, 3):\n",
    "            # Extract keypoints from the array\n",
    "            keypoints = keypoints[0, 0]\n",
    "            # Loop through each keypoint\n",
    "            for kp_index, kp in enumerate(keypoints):\n",
    "                # Extract x and y coordinates of the keypoint\n",
    "                x = int(kp[1] * frame.shape[1])\n",
    "                y = int(kp[0] * frame.shape[0])\n",
    "                # Check if the keypoint is critical\n",
    "                if keypoint_names[kp_index] in ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear']:\n",
    "                    # Calculate the average position of neighboring keypoints\n",
    "                    neighbor_indices = [c for c in connections if kp_index in c]\n",
    "                    neighbor_positions = []\n",
    "                    for connection in neighbor_indices:\n",
    "                        neighbor_kp_index = connection[0] if connection[1] == kp_index else connection[1]\n",
    "                        neighbor_positions.append(keypoints[neighbor_kp_index])\n",
    "                    neighbor_positions = np.array(neighbor_positions)\n",
    "                    average_x = int(np.mean(neighbor_positions[:, 1]) * frame.shape[1])\n",
    "                    average_y = int(np.mean(neighbor_positions[:, 0]) * frame.shape[0])\n",
    "                    # Update the position of the critical keypoint\n",
    "                    x = average_x\n",
    "                    y = average_y\n",
    "                # Draw a circle at the adjusted keypoint position\n",
    "                cv2.circle(frame, (x, y), 4, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "            # Draw lines connecting keypoints\n",
    "            for connection in connections:\n",
    "                start_point = (int(keypoints[connection[0], 1] * frame.shape[1]),\n",
    "                               int(keypoints[connection[0], 0] * frame.shape[0]))\n",
    "                end_point = (int(keypoints[connection[1], 1] * frame.shape[1]),\n",
    "                             int(keypoints[connection[1], 0] * frame.shape[0]))\n",
    "                cv2.line(frame, start_point, end_point, (0, 0, 255), 1)  # Increase thickness and change color to red\n",
    "            # Append the frame with keypoints overlay to the list\n",
    "            # print(frame.shape)\n",
    "            frames_with_keypoints.append(frame)\n",
    "        else:\n",
    "            print(\"Unexpected shape of keypoints array for frame\", frame_index + 1)\n",
    "    # Remove the last frame if it's a black frame\n",
    "    frames_with_keypoints.pop(0)\n",
    "    print(frames_with_keypoints[-1].shape)\n",
    "    if np.all(frames_with_keypoints[-1] == [0, 0, 0]):\n",
    "        frames_with_keypoints.pop()\n",
    "    # Get the frame rate from the metadata if available, otherwise use the default frame rate\n",
    "    try:\n",
    "        fps = gif.get_meta_data()['fps']\n",
    "    except KeyError:\n",
    "        fps = default_fps\n",
    "    # Save the frames with keypoints overlay as a new GIF\n",
    "    imageio.mimsave(output_gif_path, frames_with_keypoints, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_pose_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_gif_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweirddance.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to your input GIF\u001b[39;00m\n\u001b[0;32m      2\u001b[0m output_gif_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdancegrpstickk.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to save the new GIF with keypoints overlay\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sequence_keypoints \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_pose_sequence\u001b[49m(input_gif_path)\n\u001b[0;32m      4\u001b[0m visualize_and_create_pose_sequence(input_gif_path, sequence_keypoints, output_gif_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detect_pose_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "input_gif_path = \"weirddance.gif\"  # Replace with the path to your input GIF\n",
    "output_gif_path = \"dancegrpstickk.gif\"  # Path to save the new GIF with keypoints overlay\n",
    "sequence_keypoints = detect_pose_sequence(input_gif_path)\n",
    "visualize_and_create_pose_sequence(input_gif_path, sequence_keypoints, output_gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
